---
permalink: /series/HosseinAbedi/optimization/optimization-intro-1
title: بهینه‌سازی
author: Hossein Abedi
excerpt: معرفی بهینه‌سازی به همراه بررسی تعدادی از روش‌های متداول بهینه‌سازی به همراه مثال
is_cover: false
---

## سردکردن تدریجی

سردکردن تدریجی فلزات 
(Simulated Annealing)
یه الگوریتم معروف توی بهینه‌سازیه که
ایدش خیلی نسبتا سادس ولی کاربردای زیادی توی حل مساله‌های مختلف داره.
  
## ایده‌‌ی کلی روش
در این روش مثل خیلی از روش‌های دیگه‌ی بهینه‌سازی فرض بر اینه که ما می‌خواهیم فضای جستجوی مساله‌ رو طوری بگردیم که به یه نقطه‌ی بهینه‌ی سراسری برسیم. طبق معمول مشکل ما اینه که الگوریتم‌ها هیچ دانشی ممکنه از فضای جستجو نداشنه باشن و فقط در هر قدم می‌تونن چند تا ارزیابی تابع انجام بدن یعنی فقط می‌تونن تا حدی حدث بزنن که **عرصه‌ی شایستگی** (از نظر بهینگی منطقه‌ای که الان توش هستن چقدر از نواحی قبلی بهتر بوده) اطرافشون چه شکلیه. 
در صورتی هم که تابع مورد نظر چند قله‌ای باشه ( که معمولا هم هست) کلا فرار از نقاط بهینه‌ی سراری مشکل بزرگ روش‌های مورد استفادس که هر کدوم یک یا چند تا مکانیزم برای فرار از اون‌ها و رسیدن به نقاط بهینه‌ی سراسری رو استفاد می‌کنن.

![شایستگی](/assets/images/HosseinAbedi/images/opt_0.png)


این روش
 با یه **نامزد جواب** 
  به صورت تصادی شروع می‌کنه و به صورت یکنواخت فضا رو می گرده. در اول کار جواب‌هایی رو که از جواب فعلی بدتر‌ هستن رو با یه احتمالی می‌پذیره ولی به مرور زمان این احتمال که جواب‌های بدتر رو بپذیره با یه مکانیزمی کمتر می‌شه. در واقع در اول کار **جستجوی عمومی** (الگوریتم بخش زیادی از فضا رو می‌گرده)
زیادی داریم و در آخرای کار برعکس میزان جستجوی **جستجوی محلی** (جستجو بیشتر اطراف بهترین جاییه که تا حالا پیدا کردیم) خیلی زیاد می‌شه. 
  
  ويژگی‌های این روش به‌ شرح هستن:
 
## پیاده‌سازی 

کد پایتون زیر 




از نظر مفهومی می‌توان یک مساله‌ی بهینه‌سازی را به‌صورت کمینه‌ و یا بیشنه کردن یک تابع مانند 
$$f$$،
در زیر مجموعه‌ای از دامنه‌اش در نظر گرفت. این دو مساله به سادگی با تغییر علامت مقادیر خروجی تابع مورد نظر، قابل تبدیل به هم هستند.
از نظر ریاضی می‌توان مسائل بهینه‌سازی  برای کمینه‌سازی یک تابع  را به شکل‌ رسمی زیر نوشت:

\begin{align}
minimize ~f_i(x),~x\in \Re^n and~ i=1, 2,...,I
\end{align}

با داشتن شرایط زیر

\begin{align}
&h_j(x)=0,~(j=1, 2,...,J),\\
&g_k(x) \leq0,~(k=1, 2,...,K).
\end{align}


که توابع $$f$$، $$h$$ و $$g$$ معرف سه تابع هستند که  بردار ورودی  ‌آن‌ها $$x$$ می‌باشد و دو تابع آخر قیود اعمال‌شده روی $$f$$  می‌باشند. تابع $$f_i(x)$$ را در مسائل بهینه‌سازی، تابع هدف می‌نامند. در صورتی که 
$$i=1$$
باشد، مساله بهینه‌سازی تک‌هدفه و در غیر این صورت چند‌هدفه نام دارد. فصای جستجو مساله 
$$\Re^n$$
می‌باشد که درون آن به‌دنبال جواب هستیم و فضای حاصل از مقادیر خروجی توابع هدف را فضای جواب می‌نامیم. عبارات  
$$g_k$$
و 
$$h_j$$
قیود مساله بهینه‌سازی هستند که می‌خواهیم توابع هدف را تحت این قیود کمینه کنیم.
در تعدادی از مسائل،  تابع هدفی وجود ندارد و فقط با قیود روبرو هستیم در این‌گونه مسائل، هر ترکیبی از قیود می‌تواند جواب مورد نظر باشد.


##  توجه

دسته‌بندی روش‌های محاسباتی به دلیل تنوع این روش‌ها و ماهیت مساله‌هایی که می‌توان هر روش‌ را در آن اعمال نمود بسیار متنوع است. برای همین در ادامه با آوردن مثال‌هایی  از مساله‌های مختلف بهینه‌سازی به بررسی ويژگی‌های این روش‌ها و شرایط کاربرد آن‌ها خواهیم پرداخت.  


